# Stakeholder Evidence: Quality Assessment
# Evaluate the credibility and usefulness of stakeholder evidence collected

## Overall Evidence Quality Assessment

### Data Collection Quality

#### Sample Representativeness
- **Target Population:** [Total number of relevant stakeholders in organization/system]
- **Sample Size:** [Number who actually participated]
- **Response Rate:** [Percentage of those invited who participated]
- **Representativeness Score:** [High/Medium/Low] - [Explain why]

**Stakeholder Group Coverage:**
- Management: [Percentage of total sample] - Target was [percentage] 
- Employees: [Percentage of total sample] - Target was [percentage]
- Customers: [Percentage of total sample] - Target was [percentage] 
- Partners: [Percentage of total sample] - Target was [percentage]

#### Response Quality Indicators

**Survey Data Quality:**
- Complete responses: [Percentage] of total responses
- Partial responses: [Percentage] of total responses  
- Average completion time: [Minutes] (Expected: [range])
- Skip rate per question: [Average percentage]

**Interview Data Quality:**
- Average interview length: [Minutes] (Target: [range])
- Depth of responses: [High/Medium/Low] - [Explain assessment]
- Consistency across interviews: [High/Medium/Low] - [Explain assessment]

### Bias Assessment

#### Selection Bias
**Risk Level:** [High/Medium/Low]

**Self-Selection Issues:**
- Voluntary participation rate: [Percentage]
- Characteristics of non-respondents: [What you know about who didn't participate]
- Potential bias direction: [How non-participation might skew results]

**Sampling Issues:**
- Convenience sampling used: [Yes/No] - [If yes, explain limitations]
- Geographic bias: [Any location-based skewing]
- Departmental bias: [Any unit/function over/under-representation]

#### Response Bias

**Social Desirability Bias:**
- Risk assessment: [High/Medium/Low]
- Evidence of bias: [What suggests people gave "acceptable" rather than honest answers]
- Mitigation used: [How you tried to encourage honest responses]

**Acquiescence Bias:**
- Pattern of agreement: [Evidence of people just agreeing with statements]
- Question design assessment: [How well questions avoided leading responses]

**Recency/Availability Bias:**
- Recent events influence: [Whether recent events may have skewed responses]
- Typical vs. exceptional circumstances: [Whether timing affected normal perspectives]

#### Confirmation Bias (Your own)
- Question design neutrality: [How well you avoided leading questions]
- Data interpretation objectivity: [How you ensured unbiased analysis]
- Disconfirming evidence attention: [How well you looked for contradictory findings]

### Response Consistency Analysis

#### Within-Person Consistency
**Internal Consistency Checks:**
- Contradictory responses identified: [Number/percentage of respondents with inconsistent answers]
- Pattern analysis: [Common types of inconsistencies found]
- Reliability assessment: [Overall confidence in individual responses]

#### Across-Person Consistency  
**Group Agreement Levels:**
- High consensus topics: [Issues where >80% of stakeholders agreed]
- Moderate consensus topics: [Issues where 60-80% agreed]
- Low consensus topics: [Issues where <60% agreed]
- Polarized topics: [Issues with clear opposing camps]

#### Method Consistency
**Survey vs. Interview Alignment:**
- Consistent findings: [Topics where surveys and interviews agreed]
- Inconsistent findings: [Topics where methods gave different results]
- Explanation for differences: [Why methods might have yielded different results]

### Credibility Assessment by Stakeholder Group

#### Management/Leadership Input
**Credibility Score:** [High/Medium/Low]

**Strengths:**
- Strategic perspective quality: [Assessment of leadership's big-picture view]
- Resource insight accuracy: [How well leadership understands resource implications]
- Implementation realism: [How realistic leadership's assessments seem]

**Limitations:**
- Distance from problem: [How removed leadership is from day-to-day problem experience]
- Optimism bias: [Tendency to underestimate challenges]
- Political considerations: [How political factors may influence responses]

#### Employee/Staff Input  
**Credibility Score:** [High/Medium/Low]

**Strengths:**
- Direct experience authenticity: [Quality of first-hand problem experience]
- Implementation practicality: [Understanding of operational realities]
- Barrier identification accuracy: [Ability to spot real implementation obstacles]

**Limitations:**
- Limited strategic view: [Gaps in understanding broader implications]
- Change resistance: [Bias toward status quo]
- Department-specific perspective: [Views may not generalize across organization]

#### Customer/Client Input
**Credibility Score:** [High/Medium/Low]

**Strengths:** 
- Outcome focus clarity: [Clear understanding of desired results]
- External perspective value: [Insights from outside organizational dynamics]
- Impact assessment accuracy: [Good understanding of how problem affects them]

**Limitations:**
- Internal process ignorance: [Lack of understanding of organizational constraints]
- Self-interest bias: [Tendency to prioritize own needs over organizational needs]
- Limited implementation insight: [Little understanding of how solutions actually get implemented]

#### Partner/Supplier Input
**Credibility Score:** [High/Medium/Low]

**Strengths:**
- Comparative perspective: [Experience with how other organizations handle similar issues]
- Collaboration insight: [Understanding of what makes partnerships work]
- External impact awareness: [Knowledge of broader ecosystem effects]

**Limitations:**
- Conflicting interests: [Their business interests may conflict with optimal solution]
- Partial information: [Limited visibility into internal organizational dynamics]
- Relationship bias: [Tendency to maintain positive relationship rather than give hard feedback]

### Evidence Triangulation Assessment

#### Cross-Method Validation
**Survey-Interview Convergence:**
- Converging findings: [Where quantitative and qualitative data align]
- Diverging findings: [Where different methods suggest different conclusions]  
- Explanation quality: [How well you can explain any divergences]

#### Cross-Group Validation  
**Stakeholder Agreement Patterns:**
- Universal agreement: [Issues where all stakeholder groups align]
- Predictable disagreement: [Where disagreement follows expected lines (e.g., management vs. staff)]
- Surprising disagreement: [Where expected allies disagree or expected opponents agree]

### Completeness Assessment

#### Topic Coverage
- **Comprehensive topics:** [Areas where you got thorough stakeholder input]
- **Partially covered topics:** [Areas where stakeholder input was limited]
- **Missing topics:** [Areas where you didn't get stakeholder perspectives]

#### Stakeholder Voice Representation
- **Well-represented voices:** [Which stakeholder perspectives came through clearly]
- **Underrepresented voices:** [Which stakeholder perspectives were limited]
- **Missing voices:** [Which important stakeholders you couldn't reach]

### Utility Assessment for Decision-Making

#### Actionable Insights Quality
**High-Value Insights:** [Stakeholder input that clearly informs decisions]
- Specific implementation guidance: [Concrete suggestions from stakeholders]
- Barrier identification: [Clear obstacles identified by stakeholders]  
- Success factor definition: [What stakeholders say is needed for success]

**Medium-Value Insights:** [Stakeholder input that provides useful context]
- General support levels: [Overall stakeholder sentiment toward solution]
- Priority rankings: [How stakeholders prioritize different aspects]
- Resource expectations: [What stakeholders think implementation will require]

**Low-Value Insights:** [Stakeholder input that confirms obvious points]
- Predictable responses: [Feedback that matched expectations exactly]
- Vague suggestions: [Non-specific recommendations]
- Uninformed opinions: [Views from stakeholders who lack relevant knowledge]

#### Decision Support Capability
**Problem Definition Support:** [How well stakeholder evidence helps define the problem]
**Solution Design Support:** [How well stakeholder evidence informs solution design]
**Implementation Planning Support:** [How well stakeholder evidence guides implementation approach]
**Success Criteria Support:** [How well stakeholder evidence defines what success looks like]

## Overall Evidence Quality Rating

### Strengths of Stakeholder Evidence
[List the 3-5 strongest aspects of the stakeholder evidence you collected]

### Limitations of Stakeholder Evidence  
[List the 3-5 most significant limitations in your stakeholder evidence]

### Confidence Level for Decision-Making
**Overall Confidence:** [High/Medium/Low]
**Justification:** [Explain why you have this level of confidence in using this evidence for decisions]

### Recommendations for Evidence Improvement
[What you would do differently or additionally to strengthen the stakeholder evidence]

---
INSTRUCTIONS:
1. Be honest about limitations - perfect stakeholder evidence is rare
2. Consider multiple types of bias that could affect your findings
3. Assess whether different stakeholder groups' perspectives align or conflict
4. Evaluate how actionable the stakeholder insights actually are
5. Note any important stakeholder voices that are missing
